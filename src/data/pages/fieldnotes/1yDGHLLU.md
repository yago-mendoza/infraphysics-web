---
uid: "1yDGHLLU"
address: "ML//NPU"
name: "NPU"
date: "2026-02-05"
---
- Dedicated accelerator for [[7aLJOACt|ML]] inference workloads â€” optimized for matrix multiply-accumulate at low precision (INT8, FP16)
- Found inside modern [[trkh9gwv|SoC]] designs (Apple Neural Engine, Qualcomm Hexagon, Google Tensor)
- Offloads inference from the [[OkJJJyxX|CPU]] and [[WEUTQwqv|GPU]], achieving higher throughput per watt
- Programmable via frameworks like Core ML, NNAPI, or ONNX Runtime
