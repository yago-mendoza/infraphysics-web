---
uid: "MwbJnjdN"
address: "ML//BERT"
name: "BERT"
date: "2019-01-20"
---
- Bidirectional Encoder Representations from Transformers (Google, 2018)
- Encoder-only: sees the full input in both directions, unlike GPT's left-to-right
- Trained with [[1zpNyBrj|masked language modeling]] — randomly hide tokens, predict them
- Dominated NLP benchmarks for 2 years. Fine-tune on any task with a small classification head.
---
[[QtZjVPKo|Transformer]] :: encoder-only variant — proved that pre-training + fine-tuning dominates NLP
