---
uid: "0f5GJDwc"
address: "ML//Training//RLHF"
name: "RLHF"
date: "2026-02-15"
---
- Humans choosing between Bx and By — generated using different [[5qpyTXdv|temperatures]] or strategies
- The preference pairs (By > Bx) feed either [[YwfNaR4R|DPO]] directly or a [[83orykQl|reward model]] for [[rxVjxTLA|PPO]]
- InstructGPT (GPT-3.5) was the inflection point — took GPT-3 and added [[qcqxPFA0|SFT]] + RLHF, creating the first "assistant"
- The feedback loop works while humans can still judge which output is better
---
[[mCK28lZ6|constitutional AI]] :: RLAIF replaces human judges with AI judges — faster, cheaper, scales better, but depends on constitution quality
[[3EKErev3|Alignment]] :: RLHF breaks down when humans can't judge the output — the scalable oversight problem
