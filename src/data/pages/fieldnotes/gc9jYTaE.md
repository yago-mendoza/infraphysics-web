---
uid: "gc9jYTaE"
address: "ML//Alignment//AI safety"
name: "AI safety"
date: "2023-09-05"
---
- Ensuring AI systems do what we actually want, not just what we measured
- Alignment (values), robustness (edge cases), [[UHGnehtS|interpretability]] (understanding)
- The gap between "optimizes the reward function" and "does the right thing" is the entire problem
---
[[Bk7UXmQT|Goodhart's curse]] :: the central failure mode â€” optimize a proxy hard enough and it diverges from the real goal
