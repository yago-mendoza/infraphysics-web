---
uid: "j9Y9JsUA"
address: "ML//neural network//optimizer"
name: "optimizer"
date: "2017-12-03"
---
- How to update weights after computing gradients
- SGD: multiply gradient by learning rate, subtract from weight. Simple but slow
- Momentum: accumulate past gradients, like a ball rolling downhill
- [[OEOSluOX|Adam]]: adaptive learning rates per parameter â€” the default choice
