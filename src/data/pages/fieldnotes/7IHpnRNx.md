---
uid: "7IHpnRNx"
address: "ML//neural network//vanishing gradient"
name: "vanishing gradient"
date: "2018-03-08"
---
- Gradients shrink exponentially through deep layers â€” chain rule multiplies many small numbers
- Why deep nets couldn't train before ResNet skip connections
- Why [[mBCcy7bn|RNNs]] couldn't handle long sequences before [[rK1Dy2Fa|LSTM]] gates
