---
uid: "ZS34nG4d"
address: "ML//neural network//activation function//ReLU"
name: "ReLU"
date: "2017-12-03"
---
- max(0, x) â€” dead simple
- Solved the [[7IHpnRNx|vanishing gradient]] problem for deep nets: gradient is either 0 or 1, never shrinks
- "Dead neurons" downside: once a unit goes negative, it never recovers
