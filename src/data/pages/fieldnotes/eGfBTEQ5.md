---
uid: "eGfBTEQ5"
address: "ML//Transformer//positional encoding"
name: "positional encoding"
date: "2018-08-20"
---
- Transformers have no inherent sense of order — permuting the input gives the same output without this
- Original paper: sine and cosine waves at different frequencies, one per dimension
- Learned embeddings (GPT), [[m0VJ5a3l|RoPE]] (LLaMA) — the encoding method matters for long sequences
