---
uid: "wsXQoUbJ"
address: "ML//code generation//CriticGPT"
name: "CriticGPT"
date: "2026-02-26"
---
- OpenAI model trained to spot bugs in LLM-generated code
- LLMs produce code with subtle security issues, logic errors, and silent failures that pass basic tests
- CriticGPT catches errors human reviewers miss, especially in complex codebases
- Anthropic's parallel: use SAEs ([[UHGnehtS|mechanistic interpretability]]) to identify features that cause buggy generation
---
[[UHGnehtS|mechanistic interpretability]] :: Anthropic uses SAEs to find features causing buggy code â€” a direct application of interpretability to code safety
