---
address: "ML//NPU"
date: "2026-02-05"
---
Neural processing unit â€” a dedicated accelerator for [[ML]] inference workloads, optimized for matrix multiply-accumulate operations at low precision (INT8, FP16). Found inside modern [[component//chip//SoC]] designs (Apple Neural Engine, Qualcomm Hexagon, Google Tensor). Offloads inference from the [[CPU]] and [[GPU]], achieving higher throughput per watt. Programmable via frameworks like Core ML, NNAPI, or ONNX Runtime.
[[ML]]
[[component//chip//SoC]]
[[CPU]]
[[GPU]]
