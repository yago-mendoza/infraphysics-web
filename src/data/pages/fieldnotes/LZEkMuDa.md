---
uid: "LZEkMuDa"
address: "ML//Transformer//attention//cross-attention"
name: "cross-attention"
date: "2020-03-15"
---
- Q from one sequence, K and V from another
- How the decoder "looks at" the encoder output in the original Transformer
- Also how [[oZeWLvGF|DALL-E]] and [[ZY388cmd|Stable Diffusion]] condition image generation on text
