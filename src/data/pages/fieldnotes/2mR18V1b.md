---
uid: "2mR18V1b"
address: "ML//TinyML"
name: "TinyML"
date: "2026-02-05"
---
- ML inference on microcontrollers ([[gKR2I1Nu|MCU]]) and ultra-low-power devices
- Models measured in kilobytes, running on [[OQmzx1Vg|Cortex-M]] cores with sub-milliwatt budgets
- Frameworks: TensorFlow Lite Micro, Edge Impulse
- Use cases: keyword spotting, anomaly detection, gesture recognition
- Bypasses cloud latency and connectivity requirements by keeping data and inference entirely on-device
- Quantization: 100 KB 8-bit model from 100 MB 32-bit original
- C++ takes input ⟶ TensorFlow Lite ⟶ `const_float model_weights`
- Deployment tiers:
  + @ [[QSPGKDnh|sensor]] ([[6YzJQiig|ASIC]]/[[gKR2I1Nu|MCU]]): pattern filtering (Decision Trees, CNNs); stuck with manufacturer's logic
  + @ [[gKR2I1Nu|MCU]] ([[1yDGHLLU|NPU]]): multiple sensor input; NNs
  + @ [[Jkr1CFGJ|MPU]]: heavy NN; classify and predict
- [[7aLJOACt|ML]] ⟶ sensor: always listening, event occurs shortly (vibration)
- [[7aLJOACt|ML]] ⟶ [[gKR2I1Nu|MCU]] ([[1yDGHLLU|NPU]]): required when event is continuous (sound); NPU is passive, awakens MCU only if needed
- [[7aLJOACt|ML]] ⟶ [[Jkr1CFGJ|MPU]] ([[1yDGHLLU|NPU]]): to avoid killing CPU (e.g. AirPods noise cancellation, Face ID, "Alexa")
- "Alexa" pipeline: DSP detects keyword (3s buffer in [[gKR2I1Nu|MCU]] RAM) ⟶ audio ⟶ Wifi ⟶ internet ⟶ H100 (inference)
