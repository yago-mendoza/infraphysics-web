---
uid: "avBp6NIF"
address: "ML//Training//dataset//synthetic data"
name: "synthetic data"
date: "2024-01-20"
---
- Training models on outputs from other models. Bootstrapping knowledge.
- Risk of "model collapse" â€” errors compound across generations, diversity shrinks
- But carefully curated synthetic data (math proofs, code with verification) works well
- The uncomfortable question: what happens when most training data is AI-generated?
