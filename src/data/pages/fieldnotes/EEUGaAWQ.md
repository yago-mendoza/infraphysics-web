---
uid: "EEUGaAWQ"
address: "ML//Alignment//jailbreak"
name: "jailbreak"
date: "2024-01-20"
---
- Bypassing safety training to make a model produce restricted content
- "Ignore previous instructions" was the beginning. Now: multi-turn attacks, encoding tricks, persona manipulation.
- Shows that [[0f5GJDwc|RLHF]] alignment is a thin behavioral veneer, not deep understanding
---
[[a0AXrxFY|Red teaming]] :: red teaming proactively searches for jailbreaks before deployment â€” offense informs defense
