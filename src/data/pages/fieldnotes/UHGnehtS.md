---
uid: "UHGnehtS"
address: "ML//Alignment//mechanistic interpretability"
name: "mechanistic interpretability"
date: "2026-02-16"
---
- Studying what each [[RnKMoC3a|latent]] feature actually represents; trying to open the black box
- The [[3EKErev3|alignment]] approach that says "instead of trusting the model, understand it"
- Techniques: probing (test individual neurons), sparse autoencoders (decompose activations into interpretable features), activation patching (swap activations between runs to find causal features)
---
[[0f5GJDwc|RLHF]] :: interpretability reveals what [[0f5GJDwc|RLHF]] actually optimizes; whether the [[83orykQl|reward model]] captures intent or just a proxy
[[a0AXrxFY|Red teaming]] :: red teaming finds failure modes empirically (from outside); interpretability finds them mechanistically (from inside)
