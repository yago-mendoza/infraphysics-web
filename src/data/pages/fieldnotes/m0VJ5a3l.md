---
uid: "m0VJ5a3l"
address: "ML//Transformer//positional encoding//RoPE"
name: "RoPE"
date: "2023-03-20"
---
- Rotary Position Embedding â€” encode position by rotating Q and K vectors
- Relative positions emerge naturally from the angle between rotations
- Extrapolates to longer sequences better than learned absolute embeddings
- Used in LLaMA, Mistral, and most modern open LLMs
