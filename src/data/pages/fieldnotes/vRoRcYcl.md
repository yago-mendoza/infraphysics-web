---
uid: "vRoRcYcl"
address: "ML//neural network//dropout"
name: "dropout"
date: "2018-01-05"
---
- Randomly zero out neurons during training (typically 10-50%)
- Forces redundancy â€” no single neuron can be essential
- Like training an ensemble of thinner networks that share weights
