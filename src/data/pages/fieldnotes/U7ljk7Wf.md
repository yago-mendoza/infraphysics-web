---
uid: "U7ljk7Wf"
address: "ML//GPT"
name: "GPT"
date: "2026-02-15"
---
- GPT-1 (2018): [[2oNdlB5L|pre-training]] + [[qcqxPFA0|SFT]] — but SFT for specific tasks like classification, not for chatting
- GPT-2: pure pre-training — OpenAI wanted to see if it learned just from volume, no public fine-tuning
- GPT-3: pure pre-training, massive — predicted text beautifully but didn't follow instructions
- InstructGPT / GPT-3.5: the inflection point — took GPT-3 and added SFT + [[0f5GJDwc|RLHF]], birth of the "assistant"
- GPT-4 / GPT-4o: pre-training + SFT + RLHF + RLAIF — same pipeline but scaled, using AIs to help score and correct
---
[[bNGmRCsR|Training]] :: the GPT family tracks the evolution of training techniques — each generation added a new layer to the pipeline
[[0f5GJDwc|RLHF]] :: InstructGPT was the first model to use RLHF at scale — proved that human preference training transforms raw language models into useful assistants
