---
uid: "3EKErev3"
address: "ML//Alignment"
name: "Alignment"
date: "2026-02-15"
---
- The scalable oversight problem: we can't supervise what we can't understand
- The [[0f5GJDwc|RLHF]] feedback loop works while the human can still judge which output is better
- The [[mCK28lZ6|RLAIF]] feedback loop works while the human can write complex enough constitutions and trusts the model to follow them as intended
- In creating new knowledge or insight, we can't tell genius from hallucination — neither do we know how to align constitutions reliably
- The billion-dollar question: can AI reliably supervise itself without collapsing into reward hacking or echo chambers? nobody knows yet
---
[[Bk7UXmQT|Goodhart's curse]] :: even if we could scale oversight forever, the gap between proxy and true value remains
[[YwfNaR4R|DPO]] :: DPO's KL leash paradox is an alignment microcosm — remove the constraint and the model reward-hacks instead of improving
[[a0AXrxFY|Red teaming]] :: red-teaming is the empirical complement to alignment theory — find the failures, then fix them
[[UHGnehtS|mechanistic interpretability]] :: the "understand it" path to alignment; the others ([[0f5GJDwc|RLHF]], red teaming, [[mCK28lZ6|constitutional AI]]) are "control it" paths
