---
uid: "2oNdlB5L"
address: "ML//Training//Pre-training"
name: "Pre-training"
date: "2026-02-15"
---
- Imagine a baby alien staring at the static of the universe — it stares at billions of sentences until it realizes "the pilot has turned on the..." is followed by "seatbelt sign" and not "karaoke machine"
- Pure next-token prediction at massive scale — no human feedback, no instructions
- Continued pre-training: like sending a toddler who just learned to speak to medical school — dump a mountain of niche books on its head
- GPT-2 was pure pre-training (no fine-tuning) — OpenAI wanted to see if volume alone was enough
- GPT-3 was the same but massive — predicted text beautifully but didn't follow instructions
---
[[qcqxPFA0|SFT]] :: pre-training gives the model language; SFT gives it obedience
[[YwfNaR4R|DPO]] :: pre-training bakes in knowledge; DPO can only rearrange what's already there — it doesn't create new knowledge
[[U7ljk7Wf|GPT]] :: the GPT family tracks how pre-training scaled from GPT-1's modest corpus to GPT-3's internet-scale data
