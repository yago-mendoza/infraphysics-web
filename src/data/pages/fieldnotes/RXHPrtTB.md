---
uid: "RXHPrtTB"
address: "ML//neural network//activation function"
name: "activation function"
date: "2017-10-22"
---
- The non-linearity between layers
- Without it, stacking linear layers collapses to a single linear transform â€” can't learn curves
- [[ZS34nG4d|ReLU]] replaced sigmoid as the default, then [[uuSdeqbX|GELU]] took over for transformers
