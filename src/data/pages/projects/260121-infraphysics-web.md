---
id: infraphysics-web
displayTitle: "Infraphysics — building a website that thinks"
subtitle: "from blank page to knowledge graph in 18 days"
category: projects
date: 2026-01-21
thumbnail: https://images.unsplash.com/photo-1555066931-4365d14bab8c?q=80&w=400&auto=format&fit=crop
thumbnailAspect: wide
thumbnailShading: heavy
description: "How a systems engineer built a personal site with a custom markdown compiler, a second brain, and an AI development partner — and what went wrong along the way."
status: active
tags: [React, Vite, Markdown, Second Brain, Web Development]
technologies: [React, TypeScript, Vite, Tailwind CSS, Shiki, marked]
github: https://github.com/yago-mendoza/infraphysics-web
notes:
  - This is the website you're reading right now. The article, the compiler that rendered it, the theme system coloring it, the wiki-links connecting it — all of it is the project.
  - Started January 21st with a blank Vite scaffold and no web development experience worth mentioning. 18 days and 132 commits later, I had a custom markdown language, a knowledge graph with 60+ notes, and strong opinions about CSS custom properties.
  - From day 16 I worked with Claude Code (an AI coding assistant) as a development partner. The last section of the article is about what it's like to teach an LLM to maintain a codebase it didn't build — and why it keeps trying to "improve" things you didn't ask it to touch.
  - I considered Astro, I considered MDX, I considered buying a theme. I built everything from scratch instead, because the point was never to have a website — it was to find out what I didn't know.
---

>> 26.01.21 - Created the repo. No idea what I'm doing. Let's go.
>> 26.02.07 - 132 commits later, this article exists on the platform it describes. Meta.

I am not a web developer.

I build things that deal with hardware, systems, infrastructure — the kind of stuff where a misplaced byte crashes real silicon. I've spent more time reading datasheets than CSS specs. My natural habitat is closer to a [[compiler//pipeline]] than a landing page.

So when I decided to build a personal website, the rational move was to use a template. Pick a Hugo theme, write some markdown, deploy to Netlify, move on with my life. I actually considered it. I looked at Astro — which is genuinely designed for content-heavy sites with interactive islands — and even thought about buying a pre-built theme and customizing it from there. That's what a sane person would do.

I did not do the sane thing.

The truth is, I'd been circling around web development for a while. Small projects, experiments, half-finished things that never made it past the "it works on localhost" stage. And at some point the question stopped being "can I make a website" and became "can I build a _real_ one — architecture, data, interaction, everything — without leaning on someone else's decisions." Not because templates are bad. Because I wanted to know what I didn't know, and the only way to find out was to build something complex enough to punish every gap in my understanding.

So I spent 18 days building a custom React app with its own [[compiler|markdown compiler]], a wiki-style knowledge graph I call the Second Brain, a dual-theme system, and a set of scripts that refactor my notes for me. Because apparently "I just need a simple blog" is the most dangerous sentence in software engineering.

One thing worth clarifying upfront, because it's easy to misread the architecture: although the site runs on React, **the content you're reading right now was not generated by your browser.** Every article is compiled to HTML at build time — before the site is even deployed. The markdown goes through a [[compiler//pipeline|14-step pipeline]], gets transformed into static HTML, and gets served as-is. Like setting the table before the guests arrive: when you open this page, the food is already there. Your browser just sits down and eats.

> The JavaScript bundle weighs roughly 150–200 KB compressed (before React's own weight). For a personal portfolio that nobody discovers through Google, this is irrelevant. This isn't an e-commerce site competing for ad clicks. The people who find this place come from LinkedIn, GitHub, or a direct link — they already know what they're looking for.

This is the story of how it happened — what worked, what broke, and what I'd do differently if I were sane enough to do it again.

---

# Decisions and chaos

Before any code was written, there were decisions. Most of them made too quickly.

## The stack

The first question was what to build with. The honest answer is: I didn't spend a lot of time deciding. That was probably a mistake. Or maybe not.

| Option | Why I looked at it | Why I didn't pick it |
|---|---|---|
| **Hugo / Jekyll** | Fast, simple, built for blogs | Too rigid — I wanted interactive components, not just static pages |
| **Next.js** | Industry standard, SSR, file-based routing | Overkill for a personal site with no SEO needs. The server-side rendering was solving a problem I didn't have |
| **Gatsby** | React-based, plugin ecosystem | Tried it once. The plugin dependency graph felt like a build system for a build system |
| **Astro** | Lightweight, content-focused, islands architecture | Genuinely good option. Should have considered it more seriously. More below |
| **Vite + React** | Fast dev server, minimal config, I know React | Won by inertia |

### Why not Astro

If I were starting today with a clean slate and no learning agenda, Astro with islands would be the right call for the blog portion. It renders everything to static HTML by default and only activates the components you explicitly mark as interactive. The blog posts would ship zero JavaScript. The Second Brain would be an island of React inside a sea of static HTML. Architecturally elegant.

But I didn't start with a clean slate. I started with a question: --can I build a full system from scratch?-- Astro would have answered a different question — "can I pick the right tool?" — and I already knew the answer to that one. The trade-off was conscious: I chose cohesion of the codebase and simplicity of the mental model over architectural purity. One framework, one routing system, one way of thinking. Not because Astro couldn't handle it, but because the whole project asked for a single dominant logic, and React was the logic I knew.

{bkqt/tip|A note on SSG vs. SPA}
This site is often described as an SPA (Single Page Application), and technically it is — React manages the routing client-side. But the _content_ is statically generated. Every article is pre-compiled HTML served from a CDN. The distinction matters: the "SPA penalty" (slow first paint, blank screen while JavaScript loads) applies to the app shell, not to the content. And for a portfolio with no SEO requirements, the app shell penalty is negligible. The real architectural cost of using React everywhere is code weight, not rendering speed.
{/bkqt}

### Why not MDX

I also considered MDX — Markdown that lets you import React components inline. The idea was appealing: instead of writing regex-based preprocessors, I'd just write `<ColorText color="#e74c3c">danger</ColorText>` directly in my markdown.

I tried sketching it out. It didn't survive the first afternoon. The things I needed couldn't be expressed cleanly as components:

| What I needed | Syntax | Why not a component |
|---|---|---|
| Colored text | `{#e74c3c:text}` | Inline, flows with prose — XML tags interrupt it |
| Accent text | `--text--` | Takes the category color automatically, no props |
| Wiki-links | `[[address]]` | Bidirectional resolution across hundreds of files — a build-time graph operation |
| Typed blockquotes | `{bkqt/warning\|label}` | Seven types, custom labels, nested formatting |
| Context annotations | `>> 26.01.21 - text` | Relative timestamp calculation at build time |
| Backtick protection | (pipeline-level) | Extracts all code before processing, restores after — {{not a component problem|backtick protection: a system that extracts every code block and inline code span before any preprocessor runs, replaces them with placeholder tokens, processes the rest, and restores them after. Prevents custom syntax from transforming content inside code blocks.}} |

MDX solves a different problem. It lets you embed rich interactive components inside prose. What I needed was a prose-first system where custom syntax is _invisible_ — where writing `{#e74c3c:text}` feels like writing markdown, not like writing JSX. The decision wasn't dogmatic. It was practical: my syntax features are lightweight text transforms, not component trees.

### Naming things

The site has four content categories, and naming them took longer than it should have.

--Projects-- was obvious. Things I've built, with code and architecture and screenshots.

--Threads-- was harder. I wanted something that captured the feeling of a running conversation — not "articles" (too formal), not "posts" (too generic), not "essays" (too pretentious). I kept thinking about threading in systems programming — multiple execution paths sharing a context. And also, honestly, about Instagram Threads, which had just launched and was everywhere at the time. The name stuck not because of either reference specifically, but because it captures the right feeling: each thread is one train of thought that connects to others. Does it make perfect sense? Probably not. But it feels right, and in a personal project, that's enough.

--Bits2bricks-- took the longest. This section is about bridging the gap between software and physical engineering — taking what I've been learning in the digital world (bits) and bringing it back home to the world of atoms, materials, and industrial processes (bricks). A "going home" section. I'm a systems person, and the people I work with are brick engineers — they build things you can touch, things that {{obey thermodynamics|as opposed to software, which obeys whatever the developer felt like that morning.}} and don't crash silently. This section is for translating between those two worlds. The name is silly. It stays.

--Fieldnotes-- came from field notebooks — the kind engineers carry on-site to jot down observations, measurements, sketches. Each fieldnote is a concept: a node in a knowledge graph. But I'll get to that.

### The final stack

- **React 19** + **TypeScript** — because I'd rather fight the type checker than fight runtime bugs at 2am
- **Vite 6** — dev server starts in milliseconds. No webpack config. No tears
- **Tailwind CSS via CDN** — not the npm package. The CDN play mode generates classes at runtime, which means dynamic interpolation works without a build step. This matters later
- **marked** + **Shiki** — markdown parsing and syntax highlighting, both at build time
- **Cloudflare Pages** — deployment. Free, fast, global CDN

{bkqt/danger|In hindsight}
Tailwind via CDN instead of the npm package means no build-time purging — the CSS payload is larger than it needs to be. For this site's scale it doesn't matter, but it's technically debt I chose to accept. The npm version would require a PostCSS setup and rethinking how dynamic class interpolation works.
{/bkqt}

>> 26.01.21 - Initial commit. Vite scaffolding + React template. 47 files, most of them boilerplate I'll delete within a week.

## Day one: the blank page problem

The first two days were pure chaos.

I had a vague idea: a personal site with a lab section (projects, dark theme), a blog section (threads and tutorials, light theme), and a wiki-style "second brain" for my notes. Three audiences, three visual identities, one codebase. The ambition-to-skill ratio was concerning.

The initial commit was just the Vite scaffold. The second commit — "Add index" — was me staring at an empty `index.html` thinking "okay, now what." That moment where you realize that knowing _what_ you want to build and knowing _how_ to build it are two completely different skills, and you have exactly one of them.

>> 26.01.22 - Migrated from my first attempt. That one had no routing, no layout system, no plan. This one has a plan. Probably.

The third commit, two days later, was labeled "First iteration (migrated)" — I'd tried a completely different approach, hit a wall, and started over. The early commit messages tell the real story. "Solved render issue." "Bound fieldnotes list to rendering area." "Minimal shifts." Messages from someone who's discovering CSS grid for the first time and doesn't know what `overflow: hidden` does.

{bkqt/tip}
I lost an entire afternoon to a bug where my content was rendering outside its container, pushing the whole page sideways. The fix was a single CSS property: `overflow: hidden`. Web development has this quality where the bug is always trivial and the search is always endless. If you come from systems engineering, the experience is familiar — but at least in hardware, the datasheet tells you where to look. CSS has no datasheet.
{/bkqt}

### The navbar problem

The first layout had a horizontal navigation bar at the top. That's what most websites do, and it made sense when the site only had three pages. But as sections multiplied — home, about, projects, threads, bits2bricks, second brain, contact — the top bar started feeling cramped. On mobile it collapsed into a hamburger menu. On desktop it ate vertical space that should have been content.

The real issue, though, was conceptual. A top bar suggests a flat site — five or six pages at the same level. This site is not flat. It has two worlds (lab and blog) with different themes, plus a knowledge graph with its own navigation. A top bar couldn't express that hierarchy without becoming a mess.

So I moved everything to a side {{sidebar|I still don't know if "sidebar" is the right word. In hardware documentation, a sidebar is supplementary information in a callout box. In web development, it's a persistent navigation panel. I use the web meaning but it never stops feeling slightly wrong.}}. The sidebar could show the lab/blog split visually, with section groups and icons. It could also collapse on mobile without losing hierarchy. The move took a full day and broke every layout assumption I'd made up to that point. Worth it.

>> 26.01.25 - It clicked. The layout makes sense now. Routes, sidebar, content area — everything has a place. This is the first time it feels like a real project and not a homework assignment.

Then, on day four, the commit message reads: "Considerable inspired leap." I don't remember exactly what triggered it. Looking at the diff, it was the moment the responsive behavior clicked — the sidebar, the content area, the transitions between sections. The site went from "a pile of divs" to something that felt navigable. The next commit was an `App.tsx` refactor that established the routing structure I still use today. Sometimes a project crosses a threshold where it stops being an experiment and starts being a thing. This was that moment.

---

# The compiler

Here's where things got... architectural.

I needed a way to write articles in markdown and render them as styled HTML. Standard requirement. `marked` does this fine. But I also wanted colored text, subscript, superscript, keyboard shortcuts, accent text that takes the category color, typed blockquotes with labels, wiki-links to my fieldnotes, syntax highlighting with per-language themes, and context annotations with dates and relative timestamps.

Standard markdown doesn't do any of this. So I built a [[compiler//pipeline|14-step compilation pipeline]] that transforms raw markdown into the HTML you're reading right now — the same pipeline that later compiles the Second Brain fieldnotes into the knowledge graph. A language researcher would probably design this differently. But it compiles this page, and the page looks right, so I'll take it.

## The pipeline

### The ordering problem

The pipeline runs in a specific order, and --the order matters more than anything else--.

Imagine you write `{#e74c3c:some **bold** text}`. What needs to happen?

1. [[compiler//pre-processor|Pre-processors]] fire first — the color syntax wraps the content in a `<span>`
2. `marked` parses next — it sees `**bold**` inside the span and converts it to `<strong>`
3. Result: {#e74c3c:some **bold** text} — both work

If you reversed the order — ran `marked` first, then pre-processors — marked would see `{#e74c3c:...}` as literal text and leave it untouched. The braces would survive into the HTML. That would be a bug — the kind of bug that's obvious once you see it and impossible to predict if you don't.

### Backtick protection

Here's where I discovered a word that changed how I think about text processing.

A backtick. The character `` ` ``. It sits on your keyboard — you've probably pressed it {{hundreds of times|on a Spanish keyboard, it's the key right next to the P, above the + key. I'd been pressing it accidentally for years without knowing what it was called.}} without knowing its name. "Backtick" — it sounds like it should be on a menu somewhere between paella and patatas bravas. But in markdown, backticks are sacred. They mark code: `` `like this` `` for inline code, and triple backticks for code blocks. Whatever's inside them is meant to be displayed literally, not interpreted.

And here's the problem: if my pre-processors fire before `marked`, they'd also transform content inside code blocks. Writing `` `{#e74c3c:red}` `` in a tutorial would produce actual red text instead of showing the syntax. Which means I need to --protect-- everything inside backticks before any custom processing runs.

The solution is a trick I'm genuinely proud of: before any pre-processor runs, the pipeline extracts every fenced code block and inline code span, replaces them with placeholder tokens (`%%CBLK_0%%`, `%%CBLK_1%%`, ...), runs all pre-processing on the token-protected text, and then restores the original code content. Anything inside backticks is invisible to the preprocessors. They can't touch it. They don't even know it's there.

```
raw markdown
    ↓
[1] protectBackticks    →  code becomes %%CBLK_N%% tokens
[2] pre-processors      →  custom syntax transforms (safe — code is hidden)
[3] restoreBackticks    →  tokens become code again
[4] marked.parse        →  standard markdown → HTML
[5] Shiki highlighting  →  code blocks get syntax colors
```

{bkqt/tip}
If you're building a markdown extension system, protect code content first. Extract it, process everything else, restore it. Trying to make your regex "skip code blocks" is a losing game — the edge cases will eat you alive. The protection approach is conceptually simple and bulletproof in practice.
{/bkqt}

### The full pipeline

The complete compilation pipeline, in order:

| # | Step | What it does |
|---|---|---|
| 1 | `protectBackticks` | Shields code with `%%CBLK_N%%` tokens |
| 2 | `applyPreProcessors` | Color, superscript, subscript, kbd, underline, accent text |
| 3 | `processCustomBlockquotes` | `{bkqt/note}...{/bkqt}` → typed callout blocks |
| 4 | `restoreBackticks` | Brings code content back |
| 5 | `processExternalUrls` | `[[https://...]]` → external link markup (before marked to prevent URL corruption) |
| 6 | `preprocessSideImages` | Side-by-side image+text layouts |
| 7 | `processDefinitionLists` | `- TERM:: desc` → styled definition blocks |
| 8 | `processAlphabeticalLists` | `a. text` → `<ol type="a">` |
| 9 | `processContextAnnotations` | `>> 26.01.21 - text` → timestamped cards |
| 10 | `marked.parse` | Standard GFM → HTML |
| 11 | `stripHeadingFormatting` | Removes inline tags from `<h1>`–`<h4>` |
| 12 | `highlightCodeBlocks` | Shiki dual-theme highlighting per language |
| 13 | `applyPostProcessors` | Extensible HTML transforms (currently empty) |
| 14 | `processAnnotations` | `{{ref\|explanation}}` → inline footnotes |

Then two more passes across _all_ compiled files: wiki-link resolution (because link targets depend on which notes exist) and cross-document link processing.

>> 26.01.30 - The custom syntax compiler works. Color, superscript, blockquotes, wiki-links. I wrote this at 1:30pm and by 4pm I had wiki-link hover previews working. Today was a good day.

## Inventing a language

Here's the thing about custom syntax: once you start, you can't stop.

It began with colored text. I wanted to highlight key terms in category-specific colors. `{#e74c3c:danger}` → {#e74c3c:danger}. Simple enough. One regex, one `<span>`. Then superscript for exponents. Then subscript for chemical formulas. Then keyboard shortcuts. Then underline. Then accent text.

Each one was "just one more regex." And each one was trivial in isolation. But together they form a custom markup language that coexists with standard markdown — and also with the `[[wiki-links]]` that connect articles to the Second Brain. _Coexists_ is the key word. Every feature has to play nicely with bold, italic, links, code blocks, lists, tables, and every other markdown feature. The interaction surface is enormous.

{bkqt/tip|The regex tax}
Every custom syntax rule is a regex that runs on every line of every article on every build. They compose in ways you can't predict. My `_underline_` syntax uses word-boundary lookbehind to avoid matching `snake_case`, but it still conflicts with markdown's native `_italic_` syntax. I settled on `_text_` for underline (with word boundaries) and `*text*` for italic (standard markdown). The boundary is thin. If you're designing custom syntax, test every new rule against every existing one. The bugs hide in the intersections.
{/bkqt}

The blockquote system deserves its own mention. Standard markdown blockquotes (`> text`) produce small, muted text — fine for disclaimers and footnotes. But I wanted rich callout blocks with colors, icons, and labels. So I built typed blockquotes:

{bkqt/keyconcept|Typed blockquotes}
Seven types: `note` (blue), `tip` (green), `warning` (amber), `danger` (red), `keyconcept` (purple), `quote` (full-width with attribution), and `pullquote` (half-width). Custom labels override defaults. Accent text inside blockquotes adapts to the blockquote's color, not the page accent.
{/bkqt}

The syntax:

```markdown
{bkqt/warning|Memory Trap}
don't cache pointers across allocator resets.
{/bkqt}
```

This was one of the [[compiler//custom syntax]] features I couldn't stop iterating on. The first version supported three types. The current version supports seven, with custom labels, nested formatting, definition lists inside blockquotes, and color adaptation. Each iteration required rewriting the parser — blockquote content has to be processed differently from body content because definition lists and alpha lists get their own pass inside the blockquote boundaries.

{bkqt/tip|On killing features}
At some point I had an `==highlight==` syntax for emphasis. It had bugs, edge cases, and no clear use case distinct from bold or italic. I replaced it with `--accent text--`, which takes the category color automatically, looks better, and carries more semantic meaning. Subtraction can be progress. Killing a feature you built is a specific kind of satisfaction.
{/bkqt}

>> 26.02.05 - Removed `==highlight==`, replaced with `--accent--`. The double-hyphen is cleaner and doesn't conflict with anything. Killed the old code with zero nostalgia.

## The rewrite

On february 5th, the [[compiler]] broke in a way that made me realize it wasn't a bug — it was a symptom. The commit message reads: "Fixed fixes fixing fixes." When you're fixing the fixes of your previous fixes, the codebase is telling you something. It's telling you to start over.

>> 26.02.05 - "Syntax from scratch." Everything from zero. The next day, 777 lines disappeared. The codebase got lighter and I could breathe again.

The old [[compiler]] was a tangle of ad-hoc regex scattered across three files. The blockquote parser couldn't handle nested formatting. Definition lists inside blockquotes produced broken HTML. Consecutive paragraphs lost their spacing. Every fix introduced a new edge case, and every edge case demanded another fix. The interaction between the [[compiler//custom syntax]] features had become unpredictable — not because any individual feature was complex, but because they were composed without a clean boundary between them.

The rewrite centralized everything into three clean layers:

- **`compiler.config.js`** — single source of truth for all syntax rules, pre-processors, post-processors, and validation flags. Changing this file invalidates the entire build cache, which means I can't accidentally serve stale content
- **`build-content.js`** — the pipeline orchestrator. Reads the config, runs the 14 steps in order, handles caching and incremental compilation
- **`article.css`** — one stylesheet for all article styling. Category accents flow through CSS vars, not class conditionals

{bkqt/tip|On starting over}
Rewrites have a bad reputation because they're often motivated by boredom or aesthetics. This one was motivated by the [[compiler]] physically refusing to do what I needed. The difference matters: a rewrite driven by "I don't like this code" is usually a mistake. A rewrite driven by "this code can't express the next feature I need" is usually the right call. The test is whether the new version enables something the old version couldn't. Mine enabled blockquotes with nested definition lists, which the old parser literally couldn't parse.
{/bkqt}

---

# The visual layer

The site needed to look right in two modes and four colors. This is probably not how a frontend engineer would approach it — but it works, and once you understand the resolution order, it's predictable.

## Dark by default

I wanted the site to feel like a terminal. Dark background, monospace headings, lime green accents for the lab section. But the blog needed to be light — long-form reading on a dark background is a crime against eyeballs.

{bkqt/danger|In hindsight}
The first implementation was brutal: hardcoded colors everywhere. `text-white`, `bg-gray-900`, `border-gray-700`. It worked, but it meant that adding light mode later would require touching every single component. I knew I'd need a theme system eventually. Doing it "later" cost me a full day of migration. Should have set up the CSS var cascade on day one.
{/bkqt}

The solution I eventually landed on is a three-layer cascade. The idea is simple, even if the implementation has some subtlety: CSS custom properties define the actual colors, Tailwind maps those properties to semantic tokens, and components only ever use the tokens. Change one attribute on the `<html>` element and everything flips — dark to light, light to dark, no JavaScript color logic, no conditional class names.

```css
/* layer 1: CSS custom properties in index.html */
:root {
  --bg-base: #0a0a0a;
  --text-primary: rgba(255,255,255,0.87);
  --cat-projects-accent: #a3e635;   /* lime — identity color, same in both themes */
}
[data-theme="light"] {
  --bg-base: #ffffff;
  --text-primary: rgba(0,0,0,0.87);
  /* category accents don't change — they're identity, not theme */
}
```

```typescript
// layer 2: Tailwind config maps vars to semantic tokens
colors: {
  'th-base': 'var(--bg-base)',
  'th-primary': 'var(--text-primary)',
}
```

```html
<!-- layer 3: components use th-* classes -->
<div class="bg-th-base text-th-primary">
  <!-- never sees a hardcoded color -->
</div>
```

The site auto-switches theme based on route: `/lab/*` → dark (projects, Second Brain), `/blog/*` → light (threads, bits2bricks). This switch uses `useLayoutEffect` — a React hook that fires _before_ the browser paints — so the user never sees the wrong theme, even for a single frame.

> The manual toggle ({kbd:Shift+T}) works differently: a smooth fade via a CSS class called `.theme-transitioning` that temporarily enables transitions on `background-color`, `color`, and `border-color`. The auto-switch is instant (no animation), the manual toggle is smooth (fade). Two codepaths for two different intentions.

{bkqt/tip|The custom property trap}
Never put CSS custom properties in a transition list. I tried transitioning `--art-accent` directly and got a visual seizure — the browser double-interpolates, resolving the variable mid-transition while the property using it runs its own transition. The fix: only transition standard properties (`background-color`, not `--bg-base`). This cost me two hours and a headache. Literally.
{/bkqt}

There's also a light-mode asymmetry that catches you off guard if you come from dark-first design: `rgba(255,255,255,0.10)` on a dark background is clearly visible. `rgba(0,0,0,0.10)` on a white background is almost invisible. Light mode borders and surfaces need 2-3x the opacity of their dark counterparts. I forgot to adjust this. Several times.

>> 26.01.30 - Dark/light mode works. Auto-switches on route. Manual toggle with smooth fade. The hardcoded color migration took all morning but the result is clean.

## The accent cascade

Every content category has its own accent color: {#a3e635:projects} (lime), {#fb7185:threads} (rose), {#3B82F6:bits2bricks} (blue), {#a78bfa:fieldnotes} (purple). These colors need to cascade through headings, links, code block borders, blockquote bars, table headers, and dozens of other elements.

The system uses CSS custom properties with `color-mix()` derivations:

```css
/* base accent (defaults to projects/lime) */
:root { --art-accent: var(--cat-projects-accent); }

/* override per category */
.article-threads  { --art-accent: var(--cat-threads-accent); }
.article-bits2bricks { --art-accent: var(--cat-bits2bricks-accent); }

/* derivations compute from whatever --art-accent resolved to */
.article-page-wrapper {
  --art-accent-dim: color-mix(in srgb, var(--art-accent) 30%, transparent);
  --art-accent-bg: color-mix(in srgb, var(--art-accent) 5%, transparent);
}
```

The critical detail: derivations must live on `.article-page-wrapper`, not `:root`. CSS custom properties resolve at computation time, not declaration time. If the `color-mix()` derivations were on `:root`, they'd bake in the default lime accent and ignore category overrides. By placing them on the element that gets the category class, they resolve against the correct accent.

{bkqt/tip}
CSS `color-mix()` is one of those features that seems like a minor convenience until you build a theme system. Being able to write `color-mix(in srgb, var(--art-accent) 10%, transparent)` and get a tinted version of _any_ accent color — in any category, in either theme — is absurdly powerful. No JavaScript. No build step. Pure CSS.
{/bkqt}

---

# The second brain

This part of the project didn't start as a plan. It started as a folder.

I had been keeping notes — loose markdown files about CPUs, compilers, networking protocols, whatever I was studying at the time. They lived in a directory on my machine. Some referenced each other with filenames. Most didn't reference anything. There was no structure, no search, no way to see how concepts connected.

When I started building the site, those notes came with me. At first they were just another section — a list of topics you could scroll through. But the more I built, the more I realized that what I actually wanted wasn't a list. It was a graph. A place where every concept knows its neighbors, where clicking on [[CPU//ALU]] takes you to the ALU page and also shows you that ALU connects to [[CPU//core]], which connects to the [[compiler//pipeline]], which connects to the article you're reading right now.

This is the part where the project stopped being a website and started being something I actually use every day.

## The address system

{bkqt/keyconcept|The address system}
Every fieldnote has an `address` — a hierarchical identifier that doubles as its identity. `CPU//ALU` is a note about the ALU, nested under CPU. The `//` is the hierarchy separator — not `/`, which is reserved for segment names like `I/O`. The hierarchy is semantic, not organizational: it reflects how concepts relate, not how files are stored. All files live flat in one directory.
{/bkqt}

Each note is a markdown file with frontmatter:

```yaml
---
address: "CPU//ALU"
date: "2026-02-05"
aliases: [ALU, arithmetic logic unit]
---
the arithmetic logic unit — the circuit inside a [[CPU//core]] that performs...

[[CPU//core]] :: shares execution resources
[[CPU//register]]
[[CPU]]
```

The `[[wiki-links]]` in the body create references. The links at the bottom are --trailing refs-- — intentional connections that appear on both sides. If note A has a trailing ref to note B, the connection shows up on _both_ A and B's pages. One ref, bilateral display. The `::` syntax adds annotations: `[[CPU//core]] :: shares execution resources` explains _why_ the connection exists.

The bidirectional resolution is one of the things I'm most proud of technically. The build system processes every note, extracts every reference, computes the reverse links, and generates a complete relationship graph — not just a list of links, but a data structure where every connection is typed, annotated, and navigable from either end. The kind of thing you'd build with a proper graph database if you were being serious about it. I built it with JSON files and a build script. {{It works|"it works" is the systems engineer's highest compliment. Not "it's elegant." Not "it's optimized." It works.}}.

## Why this needed to be an SPA

This is where the architecture stops being a stylistic choice and becomes a functional requirement.

The Second Brain needs instant navigation between notes — you click a wiki-link, the content swaps, the URL updates, the neighborhood graph recalculates, all without a full page reload. Hover over a `[[link]]` and a floating preview appears with the target note's content. Search needs to feel instant, filtering 60+ notes as you type. The neighborhood graph needs smooth transitions when you switch contexts.

None of this works well with purely static HTML pages. Each page load would require a full round-trip, re-parsing the nav state, re-rendering the sidebar, losing scroll position. The interactivity isn't a nice-to-have — it's the core experience. The Second Brain is an application, not a document{{, and applications need client-side state management|React manages component state, re-renders on data changes, and handles routing without full page reloads. For static content it's overkill. For an interactive knowledge graph it's the minimum viable approach.}}.

Astro's islands could handle this — the blog as static HTML, the Second Brain as a React island. I acknowledged that trade-off earlier and I'll acknowledge it again here: architecturally, islands would have been more elegant. But in practice, having the Second Brain and the blog articles share the same routing, the same theme system, the same hover preview component, the same wiki-link resolution — that cohesion simplified development enormously. One mental model, one debugging strategy, one set of patterns.

## From monolith to lazy loading

{bkqt/danger|In hindsight}
The first version of the Second Brain was terrible. All notes lived in a single `_fieldnotes.md` file — one giant markdown document separated by `---`. The build parsed the whole thing and generated a single JSON blob loaded eagerly on page load. This worked at 20 notes. At 60+ it was getting slow. I should have started with one file per note from the beginning.
{/bkqt}

The current system splits content into two tiers:

- **Metadata index** (`fieldnotes-index.generated.json`) — loaded eagerly on app init. Contains addresses, references, search text. No HTML content. Small enough to be fast
- **Content files** (`public/fieldnotes/{id}.json`) — one per note. Loaded on demand when you open a note. Each file is a few KB of compiled HTML

The index gives the app everything it needs for search, navigation, and relationship display. The actual content only loads when you click. Fast initial paint, instant search, lazy content — the right trade-off.

>> 26.02.06 - Split fieldnotes into individual files. 60+ notes, each its own .md. Added incremental cache so the build only recompiles files that changed. The build went from 4 seconds to ~400ms for a single-file change.

## The neighborhood graph

Once you have hierarchical addresses and bilateral connections, you can compute neighborhoods: for any note, who are its parents, siblings, children, and connections?

The neighborhood graph is a panel that shows these relationships visually — three zones (structural hierarchy, explicit connections, mentions) with their own scroll areas and keyboard navigation. Click a node to navigate. The graph recalculates when you switch notes. Building it was straightforward since the data was already there. The tricky part was the UI: I went through four iterations of the layout before finding one that didn't feel cluttered.

## The brain sidebar

And then came the moment I needed a dedicated panel for the Second Brain — a tree view for browsing the hierarchy, with search, filters, and the active note highlighted. When I first got it working — the tree expanding, the search filtering in real time, notes appearing and disappearing as I typed — it felt like the project had crossed another threshold. Not a website anymore. A tool.

But I had a naming problem. The site already had a sidebar — the main navigation panel on the left. Now the Second Brain had its own panel too. Is it a "manager"? A "browser"? Another "sidebar"? Having two things called "sidebar" in the same codebase is confusing, especially when you don't have the webdev vocabulary to know what the convention is. I come from systems, where a "panel" is a physical thing on a rack and a "browser" is what you use to read datasheets. I ended up calling it the "Second Brain sidebar" in conversation and `SecondBrainSidebar` in code, which is descriptive if not elegant. I still don't love the name. But I love what it does.

{bkqt/tip|The render storm}
A bug worth knowing about: the wiki-link hover previews once caused the entire app to freeze. The preview component was comparing DOM element references to detect hover changes — but React recreates DOM objects on every render, so the comparison was always true, triggering an infinite re-render loop. Hundreds of renders per second until the browser gave up. The fix: compare a string attribute (the `href`) instead of the element object. Compare primitives, not objects. Four hours of debugging for a one-line fix.
{/bkqt}

---

# Keeping it alive

Building the system was one thing. Making sure it doesn't break as it grows was another.

## The safety net

Once the knowledge graph had 60+ notes with hundreds of cross-references, things started breaking in ways I couldn't see. I'd rename a concept, forget to update a reference three files away, and only discover the broken link weeks later when re-reading the note.

So I built a {{validation pipeline|runs automatically on every `npm run build`. Errors fail the build. Warnings are logged. The build is the safety net.}} that catches problems before they reach the deployed site:

| Phase | What it catches | Severity |
|---|---|---|
| 1 | Broken `[[wiki-links]]` — reference points to nonexistent note | {#e74c3c:ERROR} (fails build) |
| 2 | Self-references — a note linking to itself | {#f39c12:WARN} |
| 3 | Missing parents — `CPU//ALU` exists but `CPU` doesn't | {#f39c12:WARN} |
| 4 | Circular references — A→B→C→A cycles (opt-in) | {#f39c12:WARN} |
| 5 | Segment collisions — same concept name at different paths | {#f39c12:WARN} |
| 6 | Orphan notes — zero incoming or outgoing connections | {#22d3ee:INFO} |

Phase 5 is the most interesting. If I create `CPU//cache` and `networking//cache`, the validator flags it: "cache" appears as a leaf in two different hierarchies. Are they the same concept? (Probably — refactor into one note.) Or intentionally different? (Add `distinct: ["CPU//cache"]` to suppress the warning.) This catches a class of errors that no linter or type checker would find — conceptual duplication in a knowledge graph. It's data integrity, but for ideas.

{bkqt/keyconcept|Build as safety net}
Every command that writes content — `npm run dev`, `npm run build`, `npm run content` — runs the full validation pipeline. You should never be able to deploy a broken knowledge graph. If a reference is broken, you know within seconds, not weeks. The build is the test suite.
{/bkqt}

There's also a standalone deep audit script (`check-references.js`) that catches subtler issues: one-way trailing refs, redundant references, and fuzzy duplicates (addresses with >80% string similarity). I run it after bulk operations — creating a batch of notes, renaming hierarchies, restructuring branches.

>> 26.02.06 - The validator caught 14 broken references on its first run. Half were wiki-links in regular posts pointing to fieldnotes I'd renamed. Without the validator, those would have been dead links in production. Worth every line of code.

## Scripts that think for you

As the Second Brain grew, manual operations became error-prone. Renaming a concept means updating the note's address, renaming its file, and finding every `[[reference]]` across potentially hundreds of files. Miss one and the build breaks.

So I built a set of scripts:

- **`rename-address.js`** — renames one address and updates every reference across the entire codebase. Dry-run by default, `--apply` to execute. Atomic: either everything changes or nothing does
- **`move-hierarchy.js`** — cascading rename. Moves a note and all its descendants to a new address prefix in one operation. `"chip" → "component//chip"` automatically becomes `"chip//MCU" → "component//chip//MCU"` for every child
- **`check-references.js`** — deep audit: orphans, weak parents, one-way trailing refs, redundant refs, fuzzy duplicates, segment collisions
- **`analyze-pairs.js`** — relationship analyzer. "How are A and B connected?" Checks structural hierarchy, trailing refs, and body mentions

The rename script was born from pain. I once manually renamed a concept that had 23 references across 15 files. I missed three. The build caught two (broken `[[refs]]`). The third was a `distinct` entry in another note — not validated by the build, so it survived as a stale reference until I noticed it weeks later. After that, I wrote the script.

{bkqt/tip|The cascade trap}
`rename-address.js` renames ONE exact address. It does NOT cascade to children. If `chip` has children like `chip//MCU`, renaming `chip → component//chip` does NOT touch `chip//MCU`. That's why `move-hierarchy.js` exists — it finds every descendant and renames them all. I learned this by orphaning an entire subtree. Twice. The first time was a mistake. The second time was me not believing the first time was real.
{/bkqt}

## Teaching an AI to maintain it

On february 6th, something changed. I started using [[claude-code]] — an AI coding assistant that lives in the terminal — to help build the site. And I discovered that LLMs are simultaneously incredible and terrible at maintaining a codebase.

--Incredible-- because they can hold the entire architecture in context, suggest changes that touch five files consistently, and write code faster than I can type.

--Terrible-- because they don't remember anything between sessions. They can't read the room. They'll happily "improve" code you didn't ask them to touch, "optimize" patterns that were intentional, and "clean up" things that were there for a reason.

So I wrote a [[claude-code//dot-claude|CLAUDE.md]] — a document that lives in the repo root and contains instructions for AI assistants. Think of it as an onboarding guide for a new hire with amnesia who needs to re-read it every morning. The document has --automation rules-- — mandatory triggers that fire when specific things happen:

- File create/delete → update the README file tree
- Edit markdown in `pages/` → run the build
- Rename a fieldnote → use the rename script, never by hand
- Change the syntax pipeline → update the authoring guide
- Any code change → do ONLY what was requested

That last rule exists because AI assistants have a pathological need to "improve" adjacent code. You ask for a one-line bug fix and they refactor the entire file. The rule is there to say: --stop. Do exactly what I asked. Nothing more.--

### Hooks

Even with CLAUDE.md, the AI sometimes forgets. It edits a fieldnote and doesn't run the build. It applies a rename without dry-running first. The instructions exist, but they depend on the AI remembering to check them.

So I added [[claude-code//dot-claude//hooks|hooks]] — shell scripts that fire automatically when the AI uses specific tools. If it writes a fieldnote file, a reminder injects itself: "FIELDNOTE MODIFIED — when done, run the build." If it's about to apply a rename, a checklist appears: "did you dry-run first?"

The hooks don't block anything. They just ensure the AI sees the reminder at the right moment. It's the difference between "please remember to lock the door" (CLAUDE.md) and a sign on the door that says "DID YOU LOCK THIS?" (hooks).

{bkqt/tip}
If you work with AI coding assistants regularly, invest in guardrails. A CLAUDE.md or equivalent instructions file catches 80% of mistakes. Hooks catch the remaining 15%. The last 5% is you reading the diff before committing. None of this is bulletproof, but the combination is surprisingly effective.
{/bkqt}

>> 26.02.06 - Added CLAUDE.md, automation rules, and project configuration for Claude Code. The AI can now maintain the codebase without breaking the knowledge graph. In theory.
>> 26.02.07 - Added safety hooks. The AI no longer forgets to build after editing fieldnotes. In practice.

## Deployment

Cloudflare Pages, connected to the repo, auto-deploys on push. Images live on **Cloudflare R2** — never in git. Markdown references them by URL.

That's it. Deployment was the only part of this project that was boring.

---

This is not my domain. The CSS is definitely not how a frontend engineer would write it. The [[compiler]] is definitely not how a language designer would design it.

But it works. It renders this page. It hosts 60+ interconnected notes across four content categories. It compiles markdown through a [[compiler//pipeline|14-step pipeline]] with validation and caching. And it taught me more about web development in 18 days than any tutorial could have.

It wasn't the simplest path. It wasn't the most efficient architecture. But it was the most honest one for the kind of interaction I wanted — a portfolio as a living system, not a static showcase.

>> 26.02.07 - It's 11pm. The article is done. The compiler that rendered it is the same compiler the article describes. The wiki-links in the text point to fieldnotes that were compiled by the same pipeline. I don't know if that's clever or just recursive. I do know I'm going to keep building this thing.
