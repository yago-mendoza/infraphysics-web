{"content":"<p>Neural processing unit â€” a dedicated accelerator for <a class=\"wiki-ref\" data-address=\"ML\">ML</a> inference workloads, optimized for matrix multiply-accumulate operations at low precision (INT8, FP16). Found inside modern <a class=\"wiki-ref\" data-address=\"component//chip//SoC\">SoC</a> designs (Apple Neural Engine, Qualcomm Hexagon, Google Tensor). Offloads inference from the <a class=\"wiki-ref\" data-address=\"CPU\">CPU</a> and <a class=\"wiki-ref\" data-address=\"GPU\">GPU</a>, achieving higher throughput per watt. Programmable via frameworks like Core ML, NNAPI, or ONNX Runtime.</p>\n"}