{"content":"<p>The internal design of a <a class=\"wiki-ref\" data-address=\"GPU\">GPU</a> â€” thousands of streaming multiprocessors (NVIDIA) or compute units (AMD) grouped into warps/wavefronts that execute in lockstep. Memory hierarchy includes registers, shared memory, L1/L2 caches, and high-bandwidth GDDR or HBM. Architecture choices determine throughput for graphics, <a class=\"wiki-ref\" data-address=\"ML\">ML</a> training, and scientific workloads. <a class=\"wiki-ref\" data-address=\"GPU//discrete\">discrete</a> cards push bleeding-edge architectures; <a class=\"wiki-ref\" data-address=\"GPU//integrated\">integrated</a> units share die area with the <a class=\"wiki-ref\" data-address=\"CPU\">CPU</a>.\n<a class=\"wiki-ref\" data-address=\"GPU\">GPU</a>\n<a class=\"wiki-ref\" data-address=\"GPU//discrete\">discrete</a>\n<a class=\"wiki-ref\" data-address=\"GPU//integrated\">integrated</a>\n<a class=\"wiki-ref\" data-address=\"ML\">ML</a></p>\n"}