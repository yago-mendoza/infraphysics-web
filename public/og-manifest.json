{"/blog/bits2bricks/fpga-uart-controller":{"t":"fpga-uart-controller","d":"verilog implementation of serial comms.","img":"https://images.unsplash.com/photo-1555664424-778a69631025?q=80&w=400&auto=format&fit=crop","cat":"bits2bricks"},"/blog/bits2bricks/3d-printed-robot-arm":{"t":"3d-printed-robot-arm","d":"inverse kinematics on an arduino.","img":"https://images.unsplash.com/photo-1581091226825-a6a2a5aee158?q=80&w=400&auto=format&fit=crop","cat":"bits2bricks"},"/blog/bits2bricks/custom-syntax-pcb":{"t":"custom-syntax-pcb","d":"bridging markup syntax and hardware notation.","img":"https://images.unsplash.com/photo-1518770660439-4636190af475?q=80&w=400&auto=format&fit=crop","cat":"bits2bricks"},"/blog/bits2bricks/TEST_BITS2BRICKS":{"t":"TEST_BITS2BRICKS","d":"a technical showcase of every markdown extension — text formatting, typed blockquotes, code blocks, tables, images, and links. the accent color here is blue.","img":"https://images.unsplash.com/photo-1620712943543-bcc4688e7485?q=80&w=400&auto=format&fit=crop","cat":"bits2bricks"},"/blog/bits2bricks/openclaw-containment-playbook":{"t":"openclaw-containment-playbook","d":"Step-by-step guide to running OpenClaw safely using containers, VMs, network segmentation, and cloud deployment. Every technique ranked by effort and protection.","img":"https://images.unsplash.com/photo-1558494949-ef010cbdcc31?q=80&w=1200&auto=format&fit=crop","cat":"bits2bricks"},"/lab/projects/my-project-slug-2":{"t":"my-project-slug-2","d":"Built a local auto-scaling load balancer that spins instances, health-checks, and routes traffic based on load — shipped in ~4 hours.","img":"https://images.unsplash.com/photo-1555949963-aa79dcee981c?q=80&w=400&auto=format&fit=crop","cat":"projects"},"/lab/projects/infraphysics-web-2":{"t":"infraphysics-web-2","d":"18 days, 132 commits, one systems engineer who doesn't know CSS, and the website that somehow came out of it.","img":"https://images.unsplash.com/photo-1555066931-4365d14bab8c?q=80&w=400&auto=format&fit=crop","cat":"projects"},"/lab/projects/infraphysics-web":{"t":"infraphysics-web","d":"How a systems engineer built a personal site with a custom markdown compiler, a second brain, and an AI development partner — and what went wrong along the way.","img":"https://images.unsplash.com/photo-1555066931-4365d14bab8c?q=80&w=400&auto=format&fit=crop","cat":"projects"},"/lab/projects/finboard":{"t":"finboard","d":"A vanilla JS investment dashboard built in one session with Claude 4.6. No framework, no build step, no npm install. Just a folder you double-click.","img":"https://cdn.infraphysics.net/f7a2-k9d1-m3b8.png","cat":"projects"},"/blog/threads/anatomy-of-a-markdown-compiler":{"t":"anatomy-of-a-markdown-compiler","d":"how custom syntax coexists with standard markdown.","img":"https://images.unsplash.com/photo-1516116216517-838c2b4c4e8e?q=80&w=400&auto=format&fit=crop","cat":"threads"},"/blog/threads/TEST_THREAD":{"t":"TEST_THREAD","d":"a complete showcase of every markdown extension available in this system — text formatting, blockquotes, code blocks, images, links, and more.","img":"https://images.unsplash.com/photo-1576091160550-2173dba999ef?q=80&w=400&auto=format&fit=crop","cat":"threads"},"/blog/threads/transformers-and-the-data-wall":{"t":"transformers-and-the-data-wall","d":"Why the transformer architecture refuses to stop getting smarter — and what stands in its way.","img":"https://images.unsplash.com/photo-1620712943543-bcc4688e7485?q=80&w=400&auto=format&fit=crop","cat":"threads"},"/blog/threads/everything-is-a-pipe":{"t":"everything-is-a-pipe","d":"roman aqueducts, your bloodstream, and TCP/IP all solved the same problem. none of them knew about the others.","img":"https://images.unsplash.com/photo-1451187580459-43490279c0fa?q=80&w=400&auto=format&fit=crop","cat":"threads"},"/blog/threads/why-rust-exists":{"t":"why-rust-exists","d":"ownership, borrowing, and the compiler that won't let you hurt yourself.","img":"https://images.unsplash.com/photo-1504639725590-34d0984388bd?q=80&w=400&auto=format&fit=crop","cat":"threads"},"/blog/threads/alignment-is-not-a-vibe-check":{"t":"alignment-is-not-a-vibe-check","d":"Billions of dollars, thousands of researchers, and one very uncomfortable discovery about what happens when you teach a model to pretend.","img":"https://cdn.infraphysics.net/fkk8-sj88-8888.png","cat":"threads"},"/blog/threads/autopsia-infraphysics-web":{"t":"autopsia-infraphysics-web","d":"un análisis brutal del artículo del proyecto infraphysics-web: qué suena a IA, qué aburre, qué está desequilibrado, y cómo volverlo humano.","img":"https://images.unsplash.com/photo-1504639725590-34d0984388bd?q=80&w=400&auto=format&fit=crop","cat":"threads"},"/blog/threads/openclaw-and-the-keys-to-your-kingdom":{"t":"openclaw-and-the-keys-to-your-kingdom","d":"An AI agent with full access to your computer sounds amazing until you think about it for five minutes. This is about what that phrase actually means.","img":"https://images.unsplash.com/photo-1510511459019-5dda7724fd87?q=80&w=1200&auto=format&fit=crop","cat":"threads"},"/blog/threads/everybody-has-an-opinion-on-ai":{"t":"everybody-has-an-opinion-on-ai","d":"The real AI bubble isn't in the stocks. It's in the takes.","img":"https://images.unsplash.com/photo-1507608616759-54f48f0af0ee?q=80&w=800&auto=format&fit=crop","cat":"threads"},"/lab/second-brain/arm":{"t":"ARM","d":"A reduced instruction set computing (RISC) architecture licensed by Arm Holdings. Dominates mobile, embedded, and increasingly server workloads due to power efficiency. Unlike x86, ARM licenses its instruction set to third parties — SoC vendors like Apple, Qualcomm, and Samsung design their own cores around it. Three main profile families target different compute tiers.","img":null,"cat":"fieldnotes"},"/lab/second-brain/arm--cortex-a":{"t":"ARM//cortex-A","d":"ARM's application-profile core family — designed for high-performance workloads running full operating systems (Linux, Android, iOS). Found in smartphones, tablets, laptops (Apple M-series), and cloud servers (AWS Graviton). Features out-of-order execution, virtual memory, and multi-level caches. Pairs with integrated and NPU blocks inside a SoC.","img":null,"cat":"fieldnotes"},"/lab/second-brain/arm--cortex-m":{"t":"ARM//cortex-M","d":"ARM's microcontroller profile core family — optimized for low power, low cost, and deterministic real-time behavior. The heart of most modern MCU devices. Variants range from Cortex-M0 (tiny, 12K gates) to Cortex-M55 (with Helium vector extensions for TinyML). Executes Thumb-2 instructions, has a nested vectored interrupt controller (NVIC), and runs bare-metal or RTOS.","img":null,"cat":"fieldnotes"},"/lab/second-brain/arm--cortex-r":{"t":"ARM//cortex-R","d":"ARM's real-time profile core family — deterministic, low-latency execution for safety-critical systems. Found in automotive ECUs (braking, airbags), hard-drive controllers, and baseband modems. Features tightly-coupled memory, dual-core lockstep (fault detection), and fast interrupt response. Runs bare-metal or RTOS, not Linux. Bridges the gap between cortex-M and cortex-A.","img":null,"cat":"fieldnotes"},"/lab/second-brain/asic":{"t":"ASIC","d":"Application-specific integrated circuit — a chip designed for exactly one task. Unlike a general-purpose CPU or programmable MCU, an ASIC's logic is frozen at fab time. Delivers maximum throughput per watt for fixed workloads: Bitcoin mining, video encoding, network packet processing. TPU is a category of ML-focused ASIC. Design cost is high, so ASICs only make sense at volume.","img":null,"cat":"fieldnotes"},"/lab/second-brain/claude-code":{"t":"claude-code","d":"The AI coding assistant CLI by Anthropic. Reads project context from dot-claude, follows rules in CLAUDE.md, and executes tools (Bash, Read, Edit, Grep, etc.) autonomously. Configurable via settings, hooks, and skills.","img":null,"cat":"fieldnotes"},"/lab/second-brain/claude-code--dot-claude":{"t":"claude-code//dot-claude","d":"The `.claude/` directory at the project root. Houses all Claude Code configuration that isn't pure instructions.","img":null,"cat":"fieldnotes"},"/lab/second-brain/claude-code--dot-claude--hooks":{"t":"claude-code//dot-claude//hooks","d":"Event-driven scripts that Claude Code runs automatically at lifecycle points. Written in JavaScript (or shell), they live in `.claude/hooks/` and are referenced from settings.","img":null,"cat":"fieldnotes"},"/lab/second-brain/claude-code--dot-claude--settings":{"t":"claude-code//dot-claude//settings","d":"JSON files that control what Claude Code is allowed to do and what automations run. Two files, same format, different scope:","img":null,"cat":"fieldnotes"},"/lab/second-brain/claude-code--dot-claude--skills":{"t":"claude-code//dot-claude//skills","d":"Reusable instruction packages invoked with `/name` in a session. Each skill is a directory inside `.claude/skills/` containing a `SKILL.md` file.","img":null,"cat":"fieldnotes"},"/lab/second-brain/compiler":{"t":"compiler","d":"The build-time system that transforms raw markdown into rendered HTML. Runs headlessly via Node.js — no GUI involved. The pipeline is: pre-processors, side-image layout, marked.parse, post-processors, wiki-link injection.","img":null,"cat":"fieldnotes"},"/lab/second-brain/compiler--custom-syntax":{"t":"compiler//custom syntax","d":"Inline formatting extensions beyond standard markdown. Defined as pre-processor rules in compiler.config.js. Includes: colored text, solid/dashed/dotted/wavy underlines, highlights, superscript, subscript, keyboard keys.","img":null,"cat":"fieldnotes"},"/lab/second-brain/compiler--pipeline":{"t":"compiler//pipeline","d":"The ordered sequence of transformations applied to each markdown file. Custom syntax is resolved {_:before} marked runs, so HTML spans survive the markdown parser. Wiki-links are injected {_:after} all HTML is generated.","img":null,"cat":"fieldnotes"},"/lab/second-brain/compiler--post-processor":{"t":"compiler//post-processor","d":"A transformation rule applied to HTML {_:after} marked.parse. Currently unused but extensible — add entries to the postProcessors array in compiler.config.js.","img":null,"cat":"fieldnotes"},"/lab/second-brain/compiler--pre-processor":{"t":"compiler//pre-processor","d":"A transformation rule applied to raw markdown {_:before} marked.parse. Each rule is a regex pattern with a replacement string. Pre-processors handle custom syntax like colored text, underlines, highlights, superscripts, subscripts, and keyboard keys.","img":null,"cat":"fieldnotes"},"/lab/second-brain/component":{"t":"component","d":"A discrete functional unit — physical or logical — that can be identified, characterized, and composed with others to form a system. Components range from silicon dies (chip) to passive elements (resistors, capacitors), connectors, and software modules.","img":null,"cat":"fieldnotes"},"/lab/second-brain/component--chip":{"t":"component//chip","d":"A monolithic piece of silicon carrying integrated circuits. Categories include MCU (microcontroller — CPU + memory + peripherals), MPU (microprocessor — CPU only, needs external memory), and SoC (system-on-chip — heterogeneous integration of CPU, GPU, NPU, modems). The line between them blurs as integration density rises with each fab node shrink.","img":null,"cat":"fieldnotes"},"/lab/second-brain/component--chip--mcu":{"t":"component//chip//MCU","d":"Microcontroller unit — a self-contained chip integrating a core (often ARM Cortex-M), flash memory, RAM, and peripheral blocks (timers, ADC, UART, SPI, I2C) on a single die. Runs bare-metal or RTOS code. Examples: STM32 (STMicro), ESP32 (Espressif). MCUs dominate IoT, automotive, and industrial control where cost, power, and determinism matter more than raw speed.","img":null,"cat":"fieldnotes"},"/lab/second-brain/component--chip--mpu":{"t":"component//chip//MPU","d":"Microprocessor unit — a chip containing only the core (and caches), requiring external RAM, storage, and peripheral chips on the PCB. Desktop and server processors (Intel Core, AMD Ryzen, Apple M-series) are MPUs, though modern ones are blurring into SoC territory by integrating GPU, NPU, and memory controllers on-package.","img":null,"cat":"fieldnotes"},"/lab/second-brain/component--chip--soc":{"t":"component//chip//SoC","d":"System-on-chip — a chip integrating core clusters, GPU, NPU, memory controller, modem, and peripheral blocks onto a single die or package. Examples: Apple M-series, Qualcomm Snapdragon, NVIDIA Orin. SoCs dominate mobile and edge computing because tight integration reduces latency, power, and board area vs. discrete MPU + discrete designs.","img":null,"cat":"fieldnotes"},"/lab/second-brain/cpu":{"t":"CPU","d":"The central processing unit — the sequential instruction executor at the heart of every programmable system. Fetches, decodes, and executes one instruction stream per core. Arithmetic happens in the ALU, operands live in register files, and shared-state coordination requires mutex primitives. In embedded contexts, the CPU is often an ARM core inside a MCU or SoC.","img":null,"cat":"fieldnotes"},"/lab/second-brain/cpu--alu":{"t":"CPU//ALU","d":"The arithmetic logic unit — the combinational circuit inside a core that performs integer addition, subtraction, bitwise AND/OR/XOR, and shifts. Floating-point math typically lives in a separate FPU. Modern cores have multiple ALUs executing in parallel, fed by the out-of-order scheduler. The ALU's result is written back to a register.","img":null,"cat":"fieldnotes"},"/lab/second-brain/cpu--cache":{"t":"CPU//cache","d":"Small, fast SRAM sitting between the core and main RAM. Organized in levels: L1 (per-core, ~4 cycles), L2 (per-core or shared, ~12 cycles), L3 (shared across cores, ~40 cycles). The cache hierarchy exploits temporal and spatial locality — most programs re-access the same data and nearby addresses repeatedly. Cache misses stall the pipeline and dominate performance on memory-bound workloads. Compare with cache (disk-level) and cache (CDN/proxy-level) — same principle, different latency scale.","img":null,"cat":"fieldnotes"},"/lab/second-brain/cpu--core":{"t":"CPU//core","d":"A single, independent instruction execution pipeline within a CPU. Modern processors pack 4–128+ cores onto one die, each with private L1/L2 caches and shared L3. Multi-core scaling requires careful synchronization via mutex primitives to avoid data races. ARM big.LITTLE pairs high-performance cortex-A cores with efficient ones for power-aware scheduling.","img":null,"cat":"fieldnotes"},"/lab/second-brain/cpu--mutex":{"t":"CPU//mutex","d":"A mutual exclusion primitive — a lock that serializes access to shared RAM regions across core boundaries. Spinlocks, semaphores, and read-write locks are common variants. Contention on hot mutexes is a top scalability bottleneck. Language runtimes sometimes enforce broader exclusion: Python's GIL or JavaScript's event loop sidestep data races by constraining concurrency at the language level.","img":null,"cat":"fieldnotes"},"/lab/second-brain/cpu--mutex--event-loop":{"t":"CPU//mutex//event loop","d":"A single-threaded concurrency model — one core runs a loop that dispatches callbacks when I/O operations complete, avoiding mutex contention entirely. JavaScript (Node.js, browsers), Python asyncio, and Rust's tokio all use this pattern. Non-blocking I/O calls (epoll, kqueue, IOCP) register interest, and the loop multiplexes thousands of concurrent connections without spawning threads. Ideal for I/O-bound workloads; CPU-bound tasks block the loop and must be offloaded.","img":null,"cat":"fieldnotes"},"/lab/second-brain/cpu--mutex--gil":{"t":"CPU//mutex//GIL","d":"The Global Interpreter Lock — Python's mutex that ensures only one thread executes Python bytecode at a time. Simplifies memory management (reference counting is thread-safe without fine-grained locks) but prevents true core parallelism for CPU-bound tasks. Workarounds: multiprocessing (separate processes), C extensions that release the GIL, or async I/O via the event loop for I/O-bound code. Python 3.13+ experiments with a free-threaded build (no-GIL mode).","img":null,"cat":"fieldnotes"},"/lab/second-brain/cpu--register":{"t":"CPU//register","d":"A small, ultra-fast storage cell inside the core — typically 32 or 64 bits wide. Registers hold operands for the ALU, return addresses, stack pointers, and status flags. ARM architectures expose 16–31 general-purpose registers; x86 has historically fewer, relying more on stack and memory. Register pressure directly affects compiler optimization quality.","img":null,"cat":"fieldnotes"},"/lab/second-brain/gpu":{"t":"GPU","d":"A massively parallel processor optimized for throughput over latency. Thousands of simple cores execute the same instruction on different data (SIMD/SIMT). Originally designed for rendering, now critical for ML training and inference. Available as discrete add-in cards or integrated units sharing the SoC die with the CPU.","img":null,"cat":"fieldnotes"},"/lab/second-brain/gpu--architecture":{"t":"GPU//architecture","d":"The internal design of a GPU — thousands of streaming multiprocessors (NVIDIA) or compute units (AMD) grouped into warps/wavefronts that execute in lockstep. Memory hierarchy includes registers, shared memory, L1/L2 caches, and high-bandwidth GDDR or HBM. Architecture choices determine throughput for graphics, ML training, and scientific workloads. discrete cards push bleeding-edge architectures; integrated units share die area with the CPU.","img":null,"cat":"fieldnotes"},"/lab/second-brain/gpu--discrete":{"t":"GPU//discrete","d":"A standalone GPU on its own PCB with dedicated GDDR/HBM memory, connected to the host CPU via PCIe or NVLink. Delivers maximum compute throughput for ML training, rendering, and HPC. Power draw ranges from 75W (entry) to 700W+ (datacenter). Data must be explicitly transferred between host RAM and device memory, adding latency but offering massive bandwidth on-card.","img":null,"cat":"fieldnotes"},"/lab/second-brain/gpu--integrated":{"t":"GPU//integrated","d":"A GPU core embedded on the same die (or package) as the CPU, sharing system RAM instead of dedicated VRAM. Found in every laptop APU, mobile SoC, and Apple Silicon. Lower power and bandwidth than discrete, but zero-copy data sharing with the CPU simplifies programming and cuts latency for lightweight graphics and NPU-style inference tasks.","img":null,"cat":"fieldnotes"},"/lab/second-brain/hardware":{"t":"hardware","d":"The physical layer of computing — circuits, boards, and systems that execute software. Every abstraction in code ultimately resolves to electrons moving through semiconductor materials. A typical system stacks chip dies onto PCB boards, with CPU executing instructions, GPU handling parallel workloads, RAM providing volatile storage, and storage persisting data. The boundary between hardware and software blurs as firmware and ASIC designs encode logic that once lived in code.","img":null,"cat":"fieldnotes"},"/lab/second-brain/headless-device":{"t":"Headless device","d":"A computational unit (laptop) lacking GUI. No display subsystem, no icons, no windows. Interface limited to command line (pure input/output via keyboard and ASCII code).","img":null,"cat":"fieldnotes"},"/lab/second-brain/i-o":{"t":"I/O","d":"Input/output — the mechanisms through which a CPU communicates with the external world: storage, network, sensors, displays. Software triggers I/O via syscall (user-to-kernel boundary), while hardware uses MMIO (memory-mapped registers) or DMA (direct memory access bypassing the CPU). I/O latency often dominates real-world performance far more than raw compute speed.","img":null,"cat":"fieldnotes"},"/lab/second-brain/i-o--dma":{"t":"I/O//DMA","d":"Direct memory access — a hardware engine that copies data between RAM and peripherals without CPU intervention. The CPU programs source/dest addresses and length, then the DMA controller handles the transfer while the CPU does other work. Critical for high-throughput I/O: disk, network, audio, and discrete PCIe transfers. Embedded MCU DMA channels service ADC, SPI, and UART buffers.","img":null,"cat":"fieldnotes"},"/lab/second-brain/i-o--mmio":{"t":"I/O//MMIO","d":"Memory-mapped I/O — hardware registers mapped into the CPU's address space so they look like ordinary RAM locations. A write to address 0x4000_1000 might toggle a GPIO pin; a read from 0x4001_0004 might fetch an ADC sample. MCU programming is largely MMIO: STM32 and ESP32 SDKs provide register-level abstractions over MMIO addresses.","img":null,"cat":"fieldnotes"},"/lab/second-brain/i-o--syscall":{"t":"I/O//syscall","d":"A system call — the controlled gate between user space and kernel space. When a process needs hardware access (read file, send packet, allocate memory), it traps into the kernel via a syscall instruction. The kernel validates arguments, performs the operation, and returns. Syscall overhead (context switch + TLB flush) motivates batching APIs (io_uring, sendmmsg) and DMA for bulk transfers.","img":null,"cat":"fieldnotes"},"/lab/second-brain/ipad":{"t":"IPAD","d":"A device.","img":null,"cat":"fieldnotes"},"/lab/second-brain/ipad--ui":{"t":"IPAD//UI","d":"...","img":null,"cat":"fieldnotes"},"/lab/second-brain/laptop--ui":{"t":"LAPTOP//UI","d":"Interface channel linking human intent to machine response (voice, neural, gesture, ...).","img":null,"cat":"fieldnotes"},"/lab/second-brain/manufacturing":{"t":"manufacturing","d":"The physical processes that turn a chip design into a packaged, testable product. Spans wafer fab (lithography, etching, doping), PCB assembly (soldering components onto boards), and firmware flashing (writing initial software into non-volatile memory). Each stage introduces yield and reliability constraints.","img":null,"cat":"fieldnotes"},"/lab/second-brain/manufacturing--fab":{"t":"manufacturing//fab","d":"A semiconductor fabrication facility (foundry) that manufactures chip dies from silicon wafers. Process nodes (TSMC 3nm, Samsung 4nm, Intel 18A) define transistor density and power efficiency. Fabrication involves hundreds of lithography, etching, deposition, and doping steps. Foundries like TSMC produce ASIC, SoC, CPU, and GPU dies for fabless companies. Fab cost exceeds $20B for leading-edge nodes.","img":null,"cat":"fieldnotes"},"/lab/second-brain/manufacturing--firmware":{"t":"manufacturing//firmware","d":"The software permanently stored in a device's non-volatile memory (flash, EEPROM) — the first code that runs at power-on. For a MCU, firmware initializes clocks, configures peripheral registers via MMIO, and enters the main application loop. Written in C/C++, compiled to ARM/RISC-V machine code, and flashed onto the device via SWD, JTAG, or USB bootloader. Over-the-air (OTA) updates allow field upgrades.","img":null,"cat":"fieldnotes"},"/lab/second-brain/manufacturing--pcb":{"t":"manufacturing//PCB","d":"Printed circuit board — the fiberglass-and-copper substrate that mechanically supports and electrically connects chip components via etched traces. Design tools: KiCad, Altium. Manufacturing involves layer stackup, drilling, plating, solder mask, and silkscreen. A MCU or MPU sits on the PCB alongside passives (resistors, capacitors), connectors, and power regulation. Board complexity ranges from 2-layer hobbyist to 20+ layer server motherboards.","img":null,"cat":"fieldnotes"},"/lab/second-brain/media--image-alignment":{"t":"MEDIA//IMAGE ALIGNMENT","d":"Images can be positioned in notes.","img":null,"cat":"fieldnotes"},"/lab/second-brain/ml":{"t":"ML","d":"Machine learning — algorithms that improve through data rather than explicit programming. Training runs on GPU clusters or TPU accelerators; inference can happen on cloud GPUs, edge NPU cores, or even MCU devices via TinyML. The boundary between software and hardware blurs as ML models get baked into silicon.","img":null,"cat":"fieldnotes"},"/lab/second-brain/ml--npu":{"t":"ML//NPU","d":"Neural processing unit — a dedicated accelerator for ML inference workloads, optimized for matrix multiply-accumulate operations at low precision (INT8, FP16). Found inside modern SoC designs (Apple Neural Engine, Qualcomm Hexagon, Google Tensor). Offloads inference from the CPU and GPU, achieving higher throughput per watt. Programmable via frameworks like Core ML, NNAPI, or ONNX Runtime.","img":null,"cat":"fieldnotes"},"/lab/second-brain/ml--tinyml":{"t":"ML//TinyML","d":"Machine learning inference on microcontrollers (MCU) and ultra-low-power devices — models measured in kilobytes, running on cortex-M cores with sub-milliwatt budgets. Frameworks: TensorFlow Lite Micro, Edge Impulse. Use cases: keyword spotting, anomaly detection, gesture recognition. Bypasses cloud latency and connectivity requirements by keeping data and inference entirely on-device.","img":null,"cat":"fieldnotes"},"/lab/second-brain/ml--tpu":{"t":"ML//TPU","d":"Tensor processing unit — Google's custom ASIC designed for ML training and inference at datacenter scale. Uses a systolic array architecture to stream matrix operations with minimal memory access. TPU v4 pods interconnect thousands of chips via custom high-bandwidth networks. Exposed as cloud instances (Google Cloud TPU). Represents the extreme end of ML hardware specialization.","img":null,"cat":"fieldnotes"},"/lab/second-brain/networking":{"t":"networking","d":"The practice and infrastructure of connecting computing devices so they can exchange data. Encompasses physical media (copper, fiber, wireless), addressing schemes, and protocol stacks. Modern networking relies on packet switching and layered abstractions — from Ethernet frames to TCP segments to HTTP requests. cache (CDN, proxy) reduces latency by serving repeated content from the edge instead of the origin.","img":null,"cat":"fieldnotes"},"/lab/second-brain/networking--cache":{"t":"networking//cache","d":"A store that holds copies of frequently requested content closer to the consumer. CDNs distribute caches across edge PoPs worldwide; reverse proxies (Varnish, nginx) cache at the origin's front door. Cache invalidation — knowing when stale data must be purged — remains one of the two hard problems in computer science. The principle mirrors cache (hardware) and cache (disk), scaled up to geographic distances and HTTP semantics.","img":null,"cat":"fieldnotes"},"/lab/second-brain/peripheral":{"t":"peripheral","d":"An external or on-chip hardware block that extends a processor's capabilities — timers, UARTs, SPI, I2C, ADC, DAC, PWM controllers. In embedded development, peripherals are the interface between digital logic and the physical world. Popular development platforms like STM32 and ESP32 provide rich peripheral sets accessible via MMIO registers.","img":null,"cat":"fieldnotes"},"/lab/second-brain/peripheral--esp32":{"t":"peripheral//ESP32","d":"Espressif's Wi-Fi + Bluetooth SoC family targeting IoT. Integrates dual-core CPU (Xtensa or RISC-V), RAM, flash, radio, and peripheral blocks (ADC, SPI, I2C, PWM). Programmed via ESP-IDF (FreeRTOS-based) or Arduino. Popular for smart sensor gateways, home automation, and TinyML edge devices. Low cost (~$2–5) makes it the default prototyping platform for connected hardware.","img":null,"cat":"fieldnotes"},"/lab/second-brain/peripheral--stm32":{"t":"peripheral//STM32","d":"STMicroelectronics' family of cortex-M (and Cortex-A) based MCU and MPU devices. The STM32F4 and STM32H7 series are workhorses for embedded development — rich peripheral sets (ADC, DAC, timers, CAN, USB, Ethernet), mature tooling (STM32CubeMX, HAL drivers), and a massive community. firmware is typically C, compiled with GCC-ARM, and flashed via SWD/JTAG.","img":null,"cat":"fieldnotes"},"/lab/second-brain/ram":{"t":"RAM","d":"Random-access memory — volatile storage providing nanosecond-latency read/write for running programs. The operating system partitions physical RAM into kernel space (privileged, hardware-facing) and user space (application sandboxes). RAM bandwidth and latency dominate performance in data-intensive tasks — architecture addresses this with wide buses and high-bandwidth memory. Embedded systems often use SRAM on-chip inside a MCU.","img":null,"cat":"fieldnotes"},"/lab/second-brain/ram--kernel-space":{"t":"RAM//kernel space","d":"The protected region of RAM reserved for the operating system kernel, device drivers, and interrupt handlers. User programs cannot access kernel memory directly — they cross the boundary via syscall. Kernel space has full hardware access, including MMIO registers and DMA descriptors. A fault here crashes the entire system.","img":null,"cat":"fieldnotes"},"/lab/second-brain/ram--user-space":{"t":"RAM//user space","d":"The unprivileged region of RAM where application processes live. Each process sees a virtual address space isolated by the MMU. Communication with hardware or kernel services requires a syscall, which context-switches into kernel space. Memory allocation (malloc, mmap) operates within user space pages managed by the kernel.","img":null,"cat":"fieldnotes"},"/lab/second-brain/semiconductor":{"t":"semiconductor","d":"A material whose electrical conductivity falls between a conductor and an insulator — most commonly silicon. By selectively doping regions with impurities, engineers create P-N junctions that form transistors, the fundamental switching elements of all digital and analog electronics. The fab process etches billions of these transistors onto wafers to produce chip dies. Moore's Law has driven exponential scaling for decades, pushing from micron-scale features to sub-3nm process nodes.","img":null,"cat":"fieldnotes"},"/lab/second-brain/sensor":{"t":"sensor","d":"A device that converts a physical quantity (temperature, acceleration, light, pressure) into an electrical signal. Raw analog output is digitized by an ADC for processing. A smart sensor integrates signal conditioning and a local processor on one die. Sensors feed data to MCU or SoC platforms in IoT and industrial systems, and often connect via MMIO or serial buses.","img":null,"cat":"fieldnotes"},"/lab/second-brain/sensor--smart-sensor":{"t":"sensor//smart sensor","d":"A sensor with on-board signal conditioning, ADC, and a small processor (often cortex-M) integrated on the same die or package. Outputs calibrated digital data over I2C/SPI instead of raw analog voltage. Examples: MEMS accelerometers (Bosch BMA400), environmental sensors (BME680). Offloads filtering and fusion from the host MCU, reducing I/O traffic and CPU load.","img":null,"cat":"fieldnotes"},"/lab/second-brain/storage":{"t":"storage","d":"Persistent retention of data beyond power cycles. Ranges from embedded flash on MCU platforms to enterprise SAN arrays. The two dominant technologies are magnetic platters (HDD) and NAND flash (SSD). A cache layer absorbs hot reads and coalesces writes before they hit the slower medium. The OS abstracts raw blocks into file systems, and DMA moves bulk data without burdening the CPU.","img":null,"cat":"fieldnotes"},"/lab/second-brain/storage--cache":{"t":"storage//cache","d":"A fast buffer (usually DRAM or SLC flash) between the host and the storage medium. Write-back caches acknowledge writes before they reach the platter/NAND, boosting throughput at the cost of a power-loss risk window. Read caches keep hot blocks in memory to avoid mechanical seek or flash read latency. The OS page cache, the drive's internal DRAM, and ZFS's ARC are all layers of storage caching. Same principle as cache (nanoseconds) and cache (milliseconds), applied at the microsecond scale.","img":null,"cat":"fieldnotes"},"/lab/second-brain/syntax-reference":{"t":"SYNTAX REFERENCE","d":"Texto en **negrita**, *cursiva*, ***ambas***, y ~~tachado~~. Código en línea: `const x = 42`. _Subrayado_ reemplaza la cursiva de guiones bajos en este compilador — para cursiva se usa asterisco.","img":null,"cat":"fieldnotes"},"/lab/second-brain/test":{"t":"test","d":"The process of verifying that a system, component, or design meets its specification. Encompasses unit testing, integration testing, hardware validation (chip), and system-level qualification.","img":null,"cat":"fieldnotes"},"/lab/second-brain/test--chip":{"t":"test//chip","d":"Verification of a chip after fabrication. Includes wafer-level probe testing, package-level functional testing, and burn-in for reliability screening. Test coverage determines yield and quality — untested silicon is untrusted silicon.","img":null,"cat":"fieldnotes"},"/lab/second-brain/ui--gui":{"t":"UI//GUI","d":"UI subtype.","img":null,"cat":"fieldnotes"}}